{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local python tests\n",
    "from pprint import pprint\n",
    "from igraph import *\n",
    "import igraph.test\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import collections\n",
    "import copy\n",
    "from random import randint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    %%%%%%%%%%%%%%%%%%%%%%%%%%% gcn tests %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    x  feature vectors\n",
    "    tx feature vectors for test (unlabeled)\n",
    "    allx feature vectors for all\n",
    "\n",
    "    y  label vectors       numpy.ndarray\n",
    "    ty test label vectors  numpy.ndarray\n",
    "    ally  test label of all\n",
    "\n",
    "    graph : adjacency matrix (dict of lists)\n",
    "    test.index: node id list for tests?\n",
    "\n",
    "\n",
    "    steps\n",
    "    1) inspect ind.citeseer.x/tx/y/ty/allx/ally/test.index\n",
    "\n",
    "    2) build those files from code\n",
    "\n",
    "    3) call learning algorithm\n",
    "        cd src/notebooks\n",
    "        python gcn_tests_input.py&&  cp ind.* ../gcn/gcn/gcn/data\n",
    "        cd src/gcn/gcn/gcn\n",
    "        python train.py --dataset t1\n",
    "\n",
    "\n",
    "    4) look at embedding result\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pickleToDisk(myobj, filepath):\n",
    "    pickle.dump(myobj, open(filepath, \"wb\"))\n",
    "\n",
    "\n",
    "def printGraphObject(filepath):\n",
    "\n",
    "\n",
    "    myobject = os.path.basename(filepath)\n",
    "    print(\"\\n\\n*************** \"+myobject+\" ********************\")\n",
    "    y = pickle.load(open(filepath,'rb'))\n",
    "    print(type(y))\n",
    "    if type(y) ==  type([]) or type(y) == type(np.ndarray([])):\n",
    "        \n",
    "        print(len(y))\n",
    "        pprint(y[:min(3,len(y)-1)])\n",
    "    elif type(y) == type({}) or type(y)==type(collections.defaultdict()):\n",
    "        pprint(y.keys()[:min(3,len(y)-1)])\n",
    "        pprint(y[y.keys()[0]])\n",
    "        pprint(y[y.keys()[1]])\n",
    "        pprint(y[y.keys()[20]])\n",
    "        pprint(y[y.keys()[3000]])\n",
    "\n",
    "    else:\n",
    "        \n",
    "        pprint(y.shape)\n",
    "        pprint(y[0,:min(3,y.shape[1]-1)].toarray())\n",
    "        pprint(y[0,0].shape)\n",
    "        pprint(type(y[0,0]))\n",
    "        pprint(y[0,0])\n",
    "\n",
    "def readListFile(filepath):\n",
    "    myobject = os.path.basename(filepath)\n",
    "    print(\"\\n\\n*************** \"+myobject+\" ********************\")\n",
    "    \n",
    "\n",
    "    testInstances =[]\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            testInstances.append(int(line))\n",
    "\n",
    "    pprint(testInstances[:min(3,len(testInstances)-1)])\n",
    "\n",
    "\n",
    "def inspectData():\n",
    "\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.y')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.ty')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.x')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.tx')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.graph')\n",
    "    readListFile('../gcn/gcn/gcn/data/ind.citeseer.test.index')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.allx')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.ally')\n",
    "\n",
    "\n",
    "def discretize(elements):\n",
    "    \"\"\" create a binary array for each of the 10 ranges of values between 0 and 1\"\"\"\n",
    "    orig = np.array(elements)\n",
    "    results = []\n",
    "    \n",
    "    themax = np.amax(orig)\n",
    "    themin = np.amin(orig)\n",
    "    step = (themax - themin )/ 10.0\n",
    "    \n",
    "    for i in range(10):\n",
    "        print(i) \n",
    "        if i<9:\n",
    "            print(orig)\n",
    "            print(themin + step*i)\n",
    "            print(themin + step*(i+1))\n",
    "            range1 = np.bitwise_and(orig >= themin + step*i , orig <= themin + step*(i+1))\n",
    "        else:\n",
    "            range1 = orig >= themin + step*i\n",
    "        print(range1)\n",
    "        results.append(range1) \n",
    "        \n",
    "    result = np.transpose(np.asarray(results))\n",
    "    print(result.shape)\n",
    "    pprint(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def generateData(suffix, n_node=1000, n_test=200, n_train=300 ):\n",
    "    \n",
    "    \n",
    "\n",
    "    #igraph.test.run_tests()\n",
    "\n",
    "    # generation\n",
    "    #g = Graph.Tree(127, 2)\n",
    "    #g = Graph.GRG(100, 0.2)\n",
    "    #g = Graph.Erdos_Renyi(100, 0.2)\n",
    "    #g = Graph.Watts_Strogatz(1, 100, 4, 0.5 )\n",
    "    g = Graph.Barabasi(n_node)\n",
    "    summary(g)\n",
    "\n",
    "    # graph metrics\n",
    "    # pprint(g.degree([2,3,6,99]))\n",
    "    # pprint(g.edge_betweenness())\n",
    "    # pprint(g.pagerank())\n",
    "    #pprint(g.get_adjacency())\n",
    "    #pprint(dir(g))\n",
    "    \n",
    "\n",
    "\n",
    "    # test.index\n",
    "    testList = [] \n",
    "    with open(suffix+'.test.index','wb') as f:\n",
    "        while len(testList) < n_test:\n",
    "            j = randint(0, n_node-1)\n",
    "            #verify it is not alerady there\n",
    "            try:\n",
    "                pos = testList.index(j)\n",
    "            except:\n",
    "                testList.append(j)\n",
    "            f.write(str.encode(str(j)+\"\\n\"))\n",
    "\n",
    "\n",
    "    # adjacency dict (graph)\n",
    "    i = 0\n",
    "    A = g.get_adjlist()\n",
    "    graphAdj = {}\n",
    "    for node in A:\n",
    "        graphAdj[i] = node\n",
    "        i+=1\n",
    "    pickleToDisk(graphAdj, suffix+\".graph\")\n",
    "\n",
    "    \n",
    "    xFeatures = np.ones(n_node)\n",
    "    # allx is for labeled and unlabeled training samples\n",
    "    allx  = copy.deepcopy([xFeatures[i] for i in range(n_node) if i not in testList])\n",
    "    # x is for labeled training samples only\n",
    "    x = copy.deepcopy(allx[:-500])\n",
    "    #tx = copy.deepcopy(xFeatures[range(n_node-n_test, n_node)])\n",
    "    tx = [xFeatures[i] for i in testList]\n",
    "    \n",
    "\n",
    "    \n",
    "    allx = csr_matrix(allx)\n",
    "    allx = np.transpose(allx)\n",
    "    pickleToDisk(allx, suffix+\".allx\")\n",
    "    x = csr_matrix(x)\n",
    "    x = np.transpose(x)\n",
    "    pickleToDisk(x, suffix+\".x\")\n",
    "    tx = csr_matrix(tx)\n",
    "    tx = np.transpose(tx)\n",
    "    pickleToDisk(tx, suffix+\".tx\")\n",
    "\n",
    "    print(x.shape)\n",
    "    print(tx.shape)\n",
    "    print(allx.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    # labels (ally,y , ty)\n",
    "    prs = discretize(g.pagerank())\n",
    "\n",
    "    \n",
    "\n",
    "    # labels of x, and empty labels of unlabeled training samples\n",
    "    prs_all = copy.deepcopy([prs[i] for i in range(n_node) if i not in testList])\n",
    "    # labels of the labeled training samples\n",
    "    prs_train = copy.deepcopy(prs_all[:-500])\n",
    "    prs_test = copy.deepcopy([prs[i] for i in testList])\n",
    "    #testList.extend(validationList)\n",
    "    #finalList = testList\n",
    "    #prs_train = [ prs[i] for i in range(prs.shape[0]) if i not in finalList ]\n",
    "\n",
    "    ally = np.array(prs_all)\n",
    "    y = np.array(prs_train)  \n",
    "    ty = np.array(prs_test)\n",
    "\n",
    "\n",
    "    pickleToDisk(ally, suffix+\".ally\")\n",
    "    pickleToDisk(y, suffix+\".y\")\n",
    "    pickleToDisk(ty, suffix+\".ty\")\n",
    "    \n",
    "    print(y.shape)\n",
    "    print(ty.shape)\n",
    "    print(ally.shape)\n",
    "    pprint(ally)\n",
    "    \n",
    "\n",
    "    # run training on those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGRAPH U--- 4000 3999 -- \n",
      "(3200, 1)\n",
      "(300, 1)\n",
      "(3700, 1)\n",
      "0\n",
      "[0.00580501 0.002865   0.01689739 ... 0.00015474 0.00015398 0.00012652]\n",
      "0.00012338102411033267\n",
      "0.0018007814802592959\n",
      "[False False False ...  True  True  True]\n",
      "1\n",
      "[0.00580501 0.002865   0.01689739 ... 0.00015474 0.00015398 0.00012652]\n",
      "0.0018007814802592959\n",
      "0.003478181936408259\n",
      "[False  True False ... False False False]\n",
      "2\n",
      "[0.00580501 0.002865   0.01689739 ... 0.00015474 0.00015398 0.00012652]\n",
      "0.003478181936408259\n",
      "0.005155582392557222\n",
      "[False False False ... False False False]\n",
      "3\n",
      "[0.00580501 0.002865   0.01689739 ... 0.00015474 0.00015398 0.00012652]\n",
      "0.005155582392557222\n",
      "0.006832982848706185\n",
      "[ True False False ... False False False]\n",
      "4\n",
      "[0.00580501 0.002865   0.01689739 ... 0.00015474 0.00015398 0.00012652]\n",
      "0.006832982848706185\n",
      "0.008510383304855148\n",
      "[False False False ... False False False]\n",
      "5\n",
      "[0.00580501 0.002865   0.01689739 ... 0.00015474 0.00015398 0.00012652]\n",
      "0.008510383304855148\n",
      "0.010187783761004111\n",
      "[False False False ... False False False]\n",
      "6\n",
      "[0.00580501 0.002865   0.01689739 ... 0.00015474 0.00015398 0.00012652]\n",
      "0.010187783761004111\n",
      "0.011865184217153074\n",
      "[False False False ... False False False]\n",
      "7\n",
      "[0.00580501 0.002865   0.01689739 ... 0.00015474 0.00015398 0.00012652]\n",
      "0.011865184217153074\n",
      "0.013542584673302037\n",
      "[False False False ... False False False]\n",
      "8\n",
      "[0.00580501 0.002865   0.01689739 ... 0.00015474 0.00015398 0.00012652]\n",
      "0.013542584673302037\n",
      "0.015219985129451\n",
      "[False False False ... False False False]\n",
      "9\n",
      "[False False  True ... False False False]\n",
      "(4000, 10)\n",
      "array([[False, False, False, ..., False, False, False],\n",
      "       [False,  True, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False,  True],\n",
      "       ...,\n",
      "       [ True, False, False, ..., False, False, False],\n",
      "       [ True, False, False, ..., False, False, False],\n",
      "       [ True, False, False, ..., False, False, False]])\n",
      "(3200, 10)\n",
      "(300, 10)\n",
      "(3700, 10)\n",
      "array([[False, False, False, ..., False, False, False],\n",
      "       [False,  True, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False,  True],\n",
      "       ...,\n",
      "       [ True, False, False, ..., False, False, False],\n",
      "       [ True, False, False, ..., False, False, False],\n",
      "       [ True, False, False, ..., False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "generateData(suffix=\"ind.t3\",n_node=4000, n_train=700, n_test=300)\n",
    "!mv ind.* ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(y): 3200\n",
      "labels shape:\n",
      "(4000, 10)\n",
      "len(idx_train) 3200\n",
      "len(idx_val) 500\n",
      "len(idx_test) 310\n",
      "features after row normalize and tuple\n",
      "(array([[   0,    0],\n",
      "       [   1,    0],\n",
      "       [   2,    0],\n",
      "       ...,\n",
      "       [3997,    0],\n",
      "       [3998,    0],\n",
      "       [3999,    0]], dtype=int32),\n",
      " array([1., 1., 1., ..., 1., 1., 1.]),\n",
      " (4000, 1))\n",
      "WARNING:tensorflow:From /media/disk/home/pau/Projectes/CSN-proj/src/gcn/gcn_mod/gcn/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "2019-01-22 12:45:21.979397: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Epoch: 0001 train_loss= 2.39975 train_acc= 0.03438 val_loss= 2.33903 val_acc= 0.00000 time= 0.03901\n",
      "Epoch: 0002 train_loss= 2.36380 train_acc= 0.04375 val_loss= 2.31073 val_acc= 0.00000 time= 0.01158\n",
      "Epoch: 0003 train_loss= 2.32369 train_acc= 0.03344 val_loss= 2.28795 val_acc= 0.00000 time= 0.01112\n",
      "Epoch: 0004 train_loss= 2.29827 train_acc= 0.04188 val_loss= 2.26884 val_acc= 0.00000 time= 0.01151\n",
      "Epoch: 0005 train_loss= 2.27996 train_acc= 0.03687 val_loss= 2.25085 val_acc= 0.00000 time= 0.01197\n",
      "Epoch: 0006 train_loss= 2.26231 train_acc= 0.03750 val_loss= 2.23201 val_acc= 0.00000 time= 0.01104\n",
      "Epoch: 0007 train_loss= 2.23766 train_acc= 0.03656 val_loss= 2.21102 val_acc= 0.00000 time= 0.01073\n",
      "Epoch: 0008 train_loss= 2.22323 train_acc= 0.04938 val_loss= 2.18716 val_acc= 0.00000 time= 0.01026\n",
      "Epoch: 0009 train_loss= 2.19828 train_acc= 0.03906 val_loss= 2.15985 val_acc= 0.00000 time= 0.01107\n",
      "Epoch: 0010 train_loss= 2.16899 train_acc= 0.03594 val_loss= 2.12854 val_acc= 0.00000 time= 0.01160\n",
      "Epoch: 0011 train_loss= 2.14618 train_acc= 0.03687 val_loss= 2.09275 val_acc= 0.00000 time= 0.01132\n",
      "Epoch: 0012 train_loss= 2.11360 train_acc= 0.04719 val_loss= 2.05207 val_acc= 0.00000 time= 0.01071\n",
      "Epoch: 0013 train_loss= 2.08294 train_acc= 0.04938 val_loss= 2.00594 val_acc= 0.00000 time= 0.00999\n",
      "Epoch: 0014 train_loss= 2.05087 train_acc= 0.04781 val_loss= 1.95389 val_acc= 0.00000 time= 0.01094\n",
      "Epoch: 0015 train_loss= 1.98864 train_acc= 0.04219 val_loss= 1.89541 val_acc= 0.00000 time= 0.01199\n",
      "Epoch: 0016 train_loss= 1.94206 train_acc= 0.04250 val_loss= 1.83004 val_acc= 0.00000 time= 0.01071\n",
      "Epoch: 0017 train_loss= 1.87161 train_acc= 0.03969 val_loss= 1.75733 val_acc= 0.00000 time= 0.01122\n",
      "Epoch: 0018 train_loss= 1.82936 train_acc= 0.05500 val_loss= 1.67692 val_acc= 0.00000 time= 0.01147\n",
      "Epoch: 0019 train_loss= 1.75939 train_acc= 0.05094 val_loss= 1.58845 val_acc= 0.00000 time= 0.01004\n",
      "Epoch: 0020 train_loss= 1.67977 train_acc= 0.22156 val_loss= 1.49176 val_acc= 1.00000 time= 0.01012\n",
      "Epoch: 0021 train_loss= 1.57498 train_acc= 0.59406 val_loss= 1.38675 val_acc= 1.00000 time= 0.01053\n",
      "Epoch: 0022 train_loss= 1.50896 train_acc= 0.83156 val_loss= 1.27364 val_acc= 1.00000 time= 0.00987\n",
      "Epoch: 0023 train_loss= 1.37251 train_acc= 0.83562 val_loss= 1.15323 val_acc= 1.00000 time= 0.01111\n",
      "Epoch: 0024 train_loss= 1.29296 train_acc= 0.85812 val_loss= 1.02676 val_acc= 1.00000 time= 0.01041\n",
      "Epoch: 0025 train_loss= 1.18782 train_acc= 0.99125 val_loss= 0.89607 val_acc= 1.00000 time= 0.01036\n",
      "Epoch: 0026 train_loss= 1.07235 train_acc= 0.99063 val_loss= 0.76402 val_acc= 1.00000 time= 0.01025\n",
      "Epoch: 0027 train_loss= 0.96588 train_acc= 0.99187 val_loss= 0.63426 val_acc= 1.00000 time= 0.01188\n",
      "Epoch: 0028 train_loss= 0.94357 train_acc= 0.99219 val_loss= 0.51102 val_acc= 1.00000 time= 0.01115\n",
      "Epoch: 0029 train_loss= 0.82045 train_acc= 0.99219 val_loss= 0.39851 val_acc= 1.00000 time= 0.01058\n",
      "Epoch: 0030 train_loss= 0.73934 train_acc= 0.99156 val_loss= 0.30034 val_acc= 1.00000 time= 0.01025\n",
      "Epoch: 0031 train_loss= 0.68194 train_acc= 0.99250 val_loss= 0.21885 val_acc= 1.00000 time= 0.01014\n",
      "Epoch: 0032 train_loss= 0.60916 train_acc= 0.99250 val_loss= 0.15471 val_acc= 1.00000 time= 0.01120\n",
      "Epoch: 0033 train_loss= 0.59707 train_acc= 0.99250 val_loss= 0.10634 val_acc= 1.00000 time= 0.01121\n",
      "Epoch: 0034 train_loss= 0.51112 train_acc= 0.99250 val_loss= 0.07152 val_acc= 1.00000 time= 0.01124\n",
      "Epoch: 0035 train_loss= 0.53165 train_acc= 0.99250 val_loss= 0.04722 val_acc= 1.00000 time= 0.01042\n",
      "Epoch: 0036 train_loss= 0.47601 train_acc= 0.99250 val_loss= 0.03087 val_acc= 1.00000 time= 0.01008\n",
      "Epoch: 0037 train_loss= 0.47931 train_acc= 0.99250 val_loss= 0.02014 val_acc= 1.00000 time= 0.01029\n",
      "Epoch: 0038 train_loss= 0.42562 train_acc= 0.99250 val_loss= 0.01321 val_acc= 1.00000 time= 0.01244\n",
      "Epoch: 0039 train_loss= 0.44123 train_acc= 0.99250 val_loss= 0.00878 val_acc= 1.00000 time= 0.01074\n",
      "Epoch: 0040 train_loss= 0.44684 train_acc= 0.99250 val_loss= 0.00601 val_acc= 1.00000 time= 0.01255\n",
      "Epoch: 0041 train_loss= 0.42526 train_acc= 0.99250 val_loss= 0.00424 val_acc= 1.00000 time= 0.01047\n",
      "Epoch: 0042 train_loss= 0.44512 train_acc= 0.99250 val_loss= 0.00313 val_acc= 1.00000 time= 0.01115\n",
      "Epoch: 0043 train_loss= 0.44569 train_acc= 0.99250 val_loss= 0.00242 val_acc= 1.00000 time= 0.01144\n",
      "Epoch: 0044 train_loss= 0.42742 train_acc= 0.99250 val_loss= 0.00195 val_acc= 1.00000 time= 0.01089\n",
      "Epoch: 0045 train_loss= 0.42804 train_acc= 0.99250 val_loss= 0.00165 val_acc= 1.00000 time= 0.01154\n",
      "Epoch: 0046 train_loss= 0.45128 train_acc= 0.99250 val_loss= 0.00144 val_acc= 1.00000 time= 0.01110\n",
      "Epoch: 0047 train_loss= 0.46230 train_acc= 0.99250 val_loss= 0.00131 val_acc= 1.00000 time= 0.01307\n",
      "Epoch: 0048 train_loss= 0.40036 train_acc= 0.99250 val_loss= 0.00122 val_acc= 1.00000 time= 0.01427\n",
      "Epoch: 0049 train_loss= 0.45639 train_acc= 0.99250 val_loss= 0.00115 val_acc= 1.00000 time= 0.01227\n",
      "Epoch: 0050 train_loss= 0.39452 train_acc= 0.99250 val_loss= 0.00110 val_acc= 1.00000 time= 0.01265\n",
      "Epoch: 0051 train_loss= 0.37347 train_acc= 0.99250 val_loss= 0.00107 val_acc= 1.00000 time= 0.01168\n",
      "Epoch: 0052 train_loss= 0.39733 train_acc= 0.99250 val_loss= 0.00105 val_acc= 1.00000 time= 0.01189\n",
      "Epoch: 0053 train_loss= 0.42721 train_acc= 0.99250 val_loss= 0.00103 val_acc= 1.00000 time= 0.01255\n",
      "Epoch: 0054 train_loss= 0.40931 train_acc= 0.99250 val_loss= 0.00102 val_acc= 1.00000 time= 0.01115\n",
      "Epoch: 0055 train_loss= 0.38537 train_acc= 0.99250 val_loss= 0.00102 val_acc= 1.00000 time= 0.01180\n",
      "Epoch: 0056 train_loss= 0.41193 train_acc= 0.99250 val_loss= 0.00102 val_acc= 1.00000 time= 0.01342\n",
      "Epoch: 0057 train_loss= 0.38129 train_acc= 0.99250 val_loss= 0.00103 val_acc= 1.00000 time= 0.01304\n",
      "Epoch: 0058 train_loss= 0.39200 train_acc= 0.99250 val_loss= 0.00105 val_acc= 1.00000 time= 0.01243\n",
      "Epoch: 0059 train_loss= 0.42056 train_acc= 0.99250 val_loss= 0.00108 val_acc= 1.00000 time= 0.01302\n",
      "Early stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.08145 accuracy= 0.99333 time= 0.00445\n",
      "array([[-23.107418 , -26.596483 ],\n",
      "       [-19.424059 , -21.513273 ],\n",
      "       [-54.30077  , -59.409813 ],\n",
      "       ...,\n",
      "       [ -6.9450207, -10.421417 ],\n",
      "       [ -5.325579 ,  -7.2655034],\n",
      "       [-13.498786 , -12.995163 ]], dtype=float32)\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 1.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])\n",
      "(4000, 10)\n",
      "(4000, 10)\n",
      "(4000, 10)\n"
     ]
    }
   ],
   "source": [
    "!~/.pyenv/shims/python  train.py --dataset t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(y): 140\n",
      "labels shape:\n",
      "(2708, 7)\n",
      "len(idx_train) 140\n",
      "len(idx_val) 500\n",
      "len(idx_test) 1000\n",
      "features after row normalize and tuple\n",
      "(array([[   0, 1274],\n",
      "       [   0, 1247],\n",
      "       [   0, 1194],\n",
      "       ...,\n",
      "       [2707,  329],\n",
      "       [2707,  186],\n",
      "       [2707,   19]], dtype=int32),\n",
      " array([0.11111111, 0.11111111, 0.11111111, ..., 0.07692308, 0.07692308,\n",
      "       0.07692308], dtype=float32),\n",
      " (2708, 1433))\n",
      "WARNING:tensorflow:From /media/disk/home/pau/Projectes/CSN-proj/src/gcn/gcn_mod/gcn/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "2019-01-22 12:34:59.380341: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Epoch: 0001 train_loss= 1.95369 train_acc= 0.17857 val_loss= 1.94959 val_acc= 0.19600 time= 0.04699\n",
      "Epoch: 0002 train_loss= 1.94921 train_acc= 0.22857 val_loss= 1.94556 val_acc= 0.25000 time= 0.01850\n",
      "Epoch: 0003 train_loss= 1.94375 train_acc= 0.25714 val_loss= 1.94130 val_acc= 0.26600 time= 0.01858\n",
      "Epoch: 0004 train_loss= 1.93951 train_acc= 0.22143 val_loss= 1.93656 val_acc= 0.27000 time= 0.01966\n",
      "Epoch: 0005 train_loss= 1.93176 train_acc= 0.26429 val_loss= 1.93142 val_acc= 0.27600 time= 0.02039\n",
      "Epoch: 0006 train_loss= 1.92502 train_acc= 0.26429 val_loss= 1.92582 val_acc= 0.28800 time= 0.02206\n",
      "Epoch: 0007 train_loss= 1.91947 train_acc= 0.26429 val_loss= 1.91962 val_acc= 0.29400 time= 0.02023\n",
      "Epoch: 0008 train_loss= 1.90527 train_acc= 0.28571 val_loss= 1.91259 val_acc= 0.30000 time= 0.02057\n",
      "Epoch: 0009 train_loss= 1.90319 train_acc= 0.26429 val_loss= 1.90475 val_acc= 0.30000 time= 0.02257\n",
      "Epoch: 0010 train_loss= 1.88612 train_acc= 0.27143 val_loss= 1.89592 val_acc= 0.30400 time= 0.02151\n",
      "Epoch: 0011 train_loss= 1.87968 train_acc= 0.30714 val_loss= 1.88595 val_acc= 0.30800 time= 0.02103\n",
      "Epoch: 0012 train_loss= 1.87549 train_acc= 0.25714 val_loss= 1.87492 val_acc= 0.31200 time= 0.02085\n",
      "Epoch: 0013 train_loss= 1.85802 train_acc= 0.27143 val_loss= 1.86275 val_acc= 0.31600 time= 0.02273\n",
      "Epoch: 0014 train_loss= 1.83655 train_acc= 0.27857 val_loss= 1.84932 val_acc= 0.31400 time= 0.01897\n",
      "Epoch: 0015 train_loss= 1.82313 train_acc= 0.26429 val_loss= 1.83487 val_acc= 0.31600 time= 0.02093\n",
      "Epoch: 0016 train_loss= 1.81615 train_acc= 0.30000 val_loss= 1.81921 val_acc= 0.32800 time= 0.01925\n",
      "Epoch: 0017 train_loss= 1.76850 train_acc= 0.28571 val_loss= 1.80243 val_acc= 0.32400 time= 0.01904\n",
      "Epoch: 0018 train_loss= 1.76992 train_acc= 0.29286 val_loss= 1.78469 val_acc= 0.32200 time= 0.01925\n",
      "Epoch: 0019 train_loss= 1.76787 train_acc= 0.29286 val_loss= 1.76595 val_acc= 0.32400 time= 0.01977\n",
      "Epoch: 0020 train_loss= 1.74701 train_acc= 0.30000 val_loss= 1.74612 val_acc= 0.33000 time= 0.01813\n",
      "Epoch: 0021 train_loss= 1.70119 train_acc= 0.29286 val_loss= 1.72515 val_acc= 0.32600 time= 0.01853\n",
      "Epoch: 0022 train_loss= 1.67643 train_acc= 0.32143 val_loss= 1.70351 val_acc= 0.32200 time= 0.01842\n",
      "Epoch: 0023 train_loss= 1.64302 train_acc= 0.33571 val_loss= 1.68135 val_acc= 0.32600 time= 0.01872\n",
      "Epoch: 0024 train_loss= 1.68050 train_acc= 0.29286 val_loss= 1.65940 val_acc= 0.32400 time= 0.01979\n",
      "Epoch: 0025 train_loss= 1.62081 train_acc= 0.30714 val_loss= 1.63768 val_acc= 0.32600 time= 0.01841\n",
      "Epoch: 0026 train_loss= 1.58796 train_acc= 0.31429 val_loss= 1.61674 val_acc= 0.32800 time= 0.01932\n",
      "Epoch: 0027 train_loss= 1.60783 train_acc= 0.28571 val_loss= 1.59668 val_acc= 0.33200 time= 0.02072\n",
      "Epoch: 0028 train_loss= 1.55734 train_acc= 0.35714 val_loss= 1.57728 val_acc= 0.33200 time= 0.01907\n",
      "Epoch: 0029 train_loss= 1.53634 train_acc= 0.31429 val_loss= 1.55883 val_acc= 0.33200 time= 0.01909\n",
      "Epoch: 0030 train_loss= 1.53152 train_acc= 0.33571 val_loss= 1.54148 val_acc= 0.33600 time= 0.02099\n",
      "Epoch: 0031 train_loss= 1.48951 train_acc= 0.30714 val_loss= 1.52507 val_acc= 0.33600 time= 0.02095\n",
      "Epoch: 0032 train_loss= 1.51346 train_acc= 0.30714 val_loss= 1.50947 val_acc= 0.34400 time= 0.02143\n",
      "Epoch: 0033 train_loss= 1.47774 train_acc= 0.34286 val_loss= 1.49471 val_acc= 0.34800 time= 0.02204\n",
      "Epoch: 0034 train_loss= 1.49114 train_acc= 0.34286 val_loss= 1.48132 val_acc= 0.35000 time= 0.01912\n",
      "Epoch: 0035 train_loss= 1.46085 train_acc= 0.31429 val_loss= 1.46899 val_acc= 0.35400 time= 0.02065\n",
      "Epoch: 0036 train_loss= 1.45599 train_acc= 0.36429 val_loss= 1.45690 val_acc= 0.36000 time= 0.02043\n",
      "Epoch: 0037 train_loss= 1.40535 train_acc= 0.35714 val_loss= 1.44363 val_acc= 0.36000 time= 0.01846\n",
      "Epoch: 0038 train_loss= 1.47875 train_acc= 0.35000 val_loss= 1.43247 val_acc= 0.36600 time= 0.02183\n",
      "Epoch: 0039 train_loss= 1.38573 train_acc= 0.39286 val_loss= 1.42188 val_acc= 0.36800 time= 0.02035\n",
      "Epoch: 0040 train_loss= 1.37480 train_acc= 0.38571 val_loss= 1.41103 val_acc= 0.36800 time= 0.02204\n",
      "Epoch: 0041 train_loss= 1.37847 train_acc= 0.36429 val_loss= 1.40098 val_acc= 0.37000 time= 0.02159\n",
      "Epoch: 0042 train_loss= 1.37917 train_acc= 0.42857 val_loss= 1.39279 val_acc= 0.39200 time= 0.02061\n",
      "Epoch: 0043 train_loss= 1.35797 train_acc= 0.38571 val_loss= 1.38634 val_acc= 0.41800 time= 0.01955\n",
      "Epoch: 0044 train_loss= 1.37528 train_acc= 0.40000 val_loss= 1.38038 val_acc= 0.41400 time= 0.01903\n",
      "Epoch: 0045 train_loss= 1.32314 train_acc= 0.40000 val_loss= 1.37092 val_acc= 0.42200 time= 0.01992\n",
      "Epoch: 0046 train_loss= 1.23770 train_acc= 0.43571 val_loss= 1.36257 val_acc= 0.43200 time= 0.01864\n",
      "Epoch: 0047 train_loss= 1.27809 train_acc= 0.40000 val_loss= 1.35307 val_acc= 0.44000 time= 0.01989\n",
      "Epoch: 0048 train_loss= 1.31789 train_acc= 0.42857 val_loss= 1.34353 val_acc= 0.44400 time= 0.01977\n",
      "Epoch: 0049 train_loss= 1.24131 train_acc= 0.47857 val_loss= 1.33493 val_acc= 0.46000 time= 0.01865\n",
      "Epoch: 0050 train_loss= 1.25496 train_acc= 0.42143 val_loss= 1.32762 val_acc= 0.46400 time= 0.02040\n",
      "Epoch: 0051 train_loss= 1.21900 train_acc= 0.50000 val_loss= 1.31945 val_acc= 0.47600 time= 0.02081\n",
      "Epoch: 0052 train_loss= 1.25635 train_acc= 0.44286 val_loss= 1.31175 val_acc= 0.49000 time= 0.01798\n",
      "Epoch: 0053 train_loss= 1.20935 train_acc= 0.42857 val_loss= 1.30395 val_acc= 0.50200 time= 0.02028\n",
      "Epoch: 0054 train_loss= 1.19416 train_acc= 0.54286 val_loss= 1.29537 val_acc= 0.51600 time= 0.02060\n",
      "Epoch: 0055 train_loss= 1.23380 train_acc= 0.45714 val_loss= 1.28695 val_acc= 0.53000 time= 0.01866\n",
      "Epoch: 0056 train_loss= 1.12606 train_acc= 0.60000 val_loss= 1.27787 val_acc= 0.53600 time= 0.01910\n",
      "Epoch: 0057 train_loss= 1.17065 train_acc= 0.53571 val_loss= 1.27092 val_acc= 0.55200 time= 0.01808\n",
      "Epoch: 0058 train_loss= 1.10761 train_acc= 0.58571 val_loss= 1.26259 val_acc= 0.56400 time= 0.01850\n",
      "Epoch: 0059 train_loss= 1.16406 train_acc= 0.57857 val_loss= 1.25128 val_acc= 0.57000 time= 0.02061\n",
      "Epoch: 0060 train_loss= 1.14564 train_acc= 0.57143 val_loss= 1.24129 val_acc= 0.57800 time= 0.01995\n",
      "Epoch: 0061 train_loss= 1.11760 train_acc= 0.61429 val_loss= 1.23227 val_acc= 0.59400 time= 0.02009\n",
      "Epoch: 0062 train_loss= 1.10711 train_acc= 0.54286 val_loss= 1.22476 val_acc= 0.60800 time= 0.01826\n",
      "Epoch: 0063 train_loss= 1.14884 train_acc= 0.58571 val_loss= 1.21843 val_acc= 0.61600 time= 0.01878\n",
      "Epoch: 0064 train_loss= 1.12463 train_acc= 0.57857 val_loss= 1.21317 val_acc= 0.64400 time= 0.01963\n",
      "Epoch: 0065 train_loss= 1.05123 train_acc= 0.65714 val_loss= 1.20802 val_acc= 0.64400 time= 0.01900\n",
      "Epoch: 0066 train_loss= 1.09774 train_acc= 0.62857 val_loss= 1.20044 val_acc= 0.65000 time= 0.01838\n",
      "Epoch: 0067 train_loss= 1.12759 train_acc= 0.60000 val_loss= 1.19363 val_acc= 0.65400 time= 0.01953\n",
      "Epoch: 0068 train_loss= 1.02748 train_acc= 0.71429 val_loss= 1.18427 val_acc= 0.65200 time= 0.02078\n",
      "Epoch: 0069 train_loss= 1.13527 train_acc= 0.58571 val_loss= 1.17472 val_acc= 0.65600 time= 0.01901\n",
      "Epoch: 0070 train_loss= 1.09586 train_acc= 0.65714 val_loss= 1.16718 val_acc= 0.64400 time= 0.01963\n",
      "Epoch: 0071 train_loss= 1.00209 train_acc= 0.63571 val_loss= 1.16038 val_acc= 0.64600 time= 0.01851\n",
      "Epoch: 0072 train_loss= 1.05120 train_acc= 0.64286 val_loss= 1.15422 val_acc= 0.66200 time= 0.02077\n",
      "Epoch: 0073 train_loss= 1.13352 train_acc= 0.66429 val_loss= 1.14714 val_acc= 0.66600 time= 0.01894\n",
      "Epoch: 0074 train_loss= 1.01581 train_acc= 0.62857 val_loss= 1.14444 val_acc= 0.65800 time= 0.01827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0075 train_loss= 0.99083 train_acc= 0.62143 val_loss= 1.14228 val_acc= 0.66200 time= 0.01874\n",
      "Epoch: 0076 train_loss= 1.05695 train_acc= 0.60000 val_loss= 1.14428 val_acc= 0.66000 time= 0.01875\n",
      "Epoch: 0077 train_loss= 1.06611 train_acc= 0.62857 val_loss= 1.14551 val_acc= 0.65200 time= 0.01936\n",
      "Epoch: 0078 train_loss= 1.04987 train_acc= 0.62143 val_loss= 1.14293 val_acc= 0.65200 time= 0.01849\n",
      "Epoch: 0079 train_loss= 0.93315 train_acc= 0.70000 val_loss= 1.13907 val_acc= 0.65200 time= 0.01844\n",
      "Epoch: 0080 train_loss= 0.92226 train_acc= 0.75000 val_loss= 1.13974 val_acc= 0.64400 time= 0.01856\n",
      "Epoch: 0081 train_loss= 1.02945 train_acc= 0.61429 val_loss= 1.14003 val_acc= 0.64400 time= 0.01834\n",
      "Epoch: 0082 train_loss= 0.93219 train_acc= 0.70000 val_loss= 1.14038 val_acc= 0.64400 time= 0.01821\n",
      "Epoch: 0083 train_loss= 0.89969 train_acc= 0.72143 val_loss= 1.13858 val_acc= 0.63800 time= 0.01949\n",
      "Epoch: 0084 train_loss= 0.92550 train_acc= 0.71429 val_loss= 1.13948 val_acc= 0.64200 time= 0.01879\n",
      "Epoch: 0085 train_loss= 0.96616 train_acc= 0.70714 val_loss= 1.13364 val_acc= 0.64000 time= 0.01814\n",
      "Epoch: 0086 train_loss= 0.96750 train_acc= 0.64286 val_loss= 1.12843 val_acc= 0.63800 time= 0.01865\n",
      "Epoch: 0087 train_loss= 0.92896 train_acc= 0.65714 val_loss= 1.12707 val_acc= 0.63800 time= 0.01826\n",
      "Epoch: 0088 train_loss= 0.94086 train_acc= 0.69286 val_loss= 1.12712 val_acc= 0.63800 time= 0.01812\n",
      "Epoch: 0089 train_loss= 0.91465 train_acc= 0.68571 val_loss= 1.13078 val_acc= 0.63400 time= 0.01856\n",
      "Epoch: 0090 train_loss= 0.91973 train_acc= 0.64286 val_loss= 1.13394 val_acc= 0.63000 time= 0.01833\n",
      "Epoch: 0091 train_loss= 0.90283 train_acc= 0.67857 val_loss= 1.13664 val_acc= 0.62400 time= 0.01856\n",
      "Early stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 1.17974 accuracy= 0.61000 time= 0.00832\n",
      "array([[-5.018782 , -1.0852845],\n",
      "       [-1.1500423,  1.9082782],\n",
      "       [-3.1022313,  1.3378185],\n",
      "       ...,\n",
      "       [-3.214873 , -2.1315284],\n",
      "       [-7.0004706, -2.673078 ],\n",
      "       [-6.470511 , -2.3801403]], dtype=float32)\n",
      "(2708, 7)\n",
      "(2708, 7)\n",
      "(2708, 7)\n"
     ]
    }
   ],
   "source": [
    "!~/.pyenv/shims/python  train.py --dataset cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot2DEmbedding(filepath, labelspath):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    import matplotlib.lines as mlines\n",
    "    import matplotlib.cm as cm\n",
    "    %matplotlib inline\n",
    "\n",
    "    # read embedding labels file\n",
    "    with open(labelspath,\"r\") as f:\n",
    "        size = len(f.readlines())\n",
    "        labels = np.zeros((size,1))\n",
    "\n",
    "    # read embedding file\n",
    "    with open(filepath,\"r\") as f:\n",
    "        size = len(f.readlines())\n",
    "\n",
    "        \n",
    "    # read embedding labels file\n",
    "    embeddings = {}\n",
    "    with open(labelspath,\"r\") as f:\n",
    "        i=0\n",
    "        for label in f.readlines():\n",
    "            #print(line)\n",
    "            vals = label.split()\n",
    "            label_int = 0\n",
    "            for j in range(len(vals)):\n",
    "                label_int += float(vals[j])*j\n",
    "            labels[i]=int(label_int)\n",
    "            i=i+1\n",
    "            \n",
    "        labels_list = [ lab[0] for lab in labels.tolist()]\n",
    "        for j in set(labels_list):\n",
    "            embeddings[j]=np.empty((0,2))\n",
    "            \n",
    "        with open(filepath,\"r\") as f2:\n",
    "            i = 0\n",
    "            for line in f2.readlines():\n",
    "                #print(line)\n",
    "                d1,d2 = line.split()\n",
    "                key = int(labels_list[i])\n",
    "                embedding = embeddings[key]\n",
    "                \n",
    "                embeddings[key] = np.append(embedding, [[float(d1), float(d2)]], axis=0)\n",
    "                #embedding[i,0]=float(d1)\n",
    "                #embedding[i,1]=float(d2)\n",
    "                i=i+1\n",
    "        \n",
    "        # separate embedding by label\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        color_labels = [int(k % 23) for k in set(labels_list) ]\n",
    "        colors = cm.rainbow(np.linspace(0, 1, len(color_labels)))\n",
    "        for colref,col in zip(colors, color_labels) :\n",
    "            #print(col)\n",
    "            #print([[col]*embeddings[col].shape[0]])\n",
    "            #print(embeddings[col].shape[0])\n",
    "            plt.scatter(embeddings[col][:,0], embeddings[col][:,1], c=[colref]*embeddings[col].shape[0], label=str(col))\n",
    "        #col=0\n",
    "        #plt.scatter(embeddings[col][:,0], embeddings[col][:,1], c=str(col), label=str(col))\n",
    "        plt.suptitle('Embedding')\n",
    "        plt.xlabel('dim 1')\n",
    "        plt.ylabel('dim 2')\n",
    "        plt.legend()\n",
    "        \n",
    "        \n",
    "        # histogram of labels\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(labels_list, color='blue')\n",
    "        plt.subplots_adjust(wspace=0.5)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "   \n",
    "        \n",
    "plot2DEmbedding(\"embedding\",\"embedding_labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csnProjPy3",
   "language": "python",
   "name": "csnprojpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
