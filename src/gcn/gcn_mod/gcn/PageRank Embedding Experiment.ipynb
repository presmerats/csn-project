{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local python tests\n",
    "from pprint import pprint\n",
    "from igraph import *\n",
    "import igraph.test\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import collections\n",
    "import copy\n",
    "from random import randint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    %%%%%%%%%%%%%%%%%%%%%%%%%%% gcn tests %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    x  feature vectors\n",
    "    tx feature vectors for test (unlabeled)\n",
    "    allx feature vectors for all\n",
    "\n",
    "    y  label vectors       numpy.ndarray\n",
    "    ty test label vectors  numpy.ndarray\n",
    "    ally  test label of all\n",
    "\n",
    "    graph : adjacency matrix (dict of lists)\n",
    "    test.index: node id list for tests?\n",
    "\n",
    "\n",
    "    steps\n",
    "    1) inspect ind.citeseer.x/tx/y/ty/allx/ally/test.index\n",
    "\n",
    "    2) build those files from code\n",
    "\n",
    "    3) call learning algorithm\n",
    "        cd src/notebooks\n",
    "        python gcn_tests_input.py&&  cp ind.* ../gcn/gcn/gcn/data\n",
    "        cd src/gcn/gcn/gcn\n",
    "        python train.py --dataset t1\n",
    "\n",
    "\n",
    "    4) look at embedding result\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pickleToDisk(myobj, filepath):\n",
    "    pickle.dump(myobj, open(filepath, \"wb\"))\n",
    "\n",
    "\n",
    "def printGraphObject(filepath):\n",
    "\n",
    "\n",
    "    myobject = os.path.basename(filepath)\n",
    "    print(\"\\n\\n*************** \"+myobject+\" ********************\")\n",
    "    y = pickle.load(open(filepath,'rb'))\n",
    "    print(type(y))\n",
    "    if type(y) ==  type([]) or type(y) == type(np.ndarray([])):\n",
    "        \n",
    "        print(len(y))\n",
    "        pprint(y[:min(3,len(y)-1)])\n",
    "    elif type(y) == type({}) or type(y)==type(collections.defaultdict()):\n",
    "        pprint(y.keys()[:min(3,len(y)-1)])\n",
    "        pprint(y[y.keys()[0]])\n",
    "        pprint(y[y.keys()[1]])\n",
    "        pprint(y[y.keys()[20]])\n",
    "        pprint(y[y.keys()[3000]])\n",
    "\n",
    "    else:\n",
    "        \n",
    "        pprint(y.shape)\n",
    "        pprint(y[0,:min(3,y.shape[1]-1)].toarray())\n",
    "        pprint(y[0,0].shape)\n",
    "        pprint(type(y[0,0]))\n",
    "        pprint(y[0,0])\n",
    "\n",
    "def readListFile(filepath):\n",
    "    myobject = os.path.basename(filepath)\n",
    "    print(\"\\n\\n*************** \"+myobject+\" ********************\")\n",
    "    \n",
    "\n",
    "    testInstances =[]\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            testInstances.append(int(line))\n",
    "\n",
    "    pprint(testInstances[:min(3,len(testInstances)-1)])\n",
    "\n",
    "\n",
    "def inspectData():\n",
    "\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.y')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.ty')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.x')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.tx')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.graph')\n",
    "    readListFile('../gcn/gcn/gcn/data/ind.citeseer.test.index')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.allx')\n",
    "    printGraphObject('../gcn/gcn/gcn/data/ind.citeseer.ally')\n",
    "\n",
    "\n",
    "def discretize(elements):\n",
    "    \"\"\" create a binary array for each of the 10 ranges of values between 0 and 1\"\"\"\n",
    "    orig = np.array(elements)\n",
    "    results = []\n",
    "    for i in range(10):\n",
    "        print(i) \n",
    "        if i<9:\n",
    "            range1 = np.bitwise_and(orig >= 0.1*i ,orig <= 0.1*(i+1))\n",
    "        else:\n",
    "            range1 = orig >= 0.1*i\n",
    "        print(range1)\n",
    "        results.append(range1) \n",
    "        \n",
    "    result = np.transpose(np.asarray(results))\n",
    "    print(result.shape)\n",
    "    return result\n",
    "\n",
    "\n",
    "def generateData(suffix, n_node=1000, n_test=200, n_train=300 ):\n",
    "    \n",
    "    \n",
    "\n",
    "    #igraph.test.run_tests()\n",
    "\n",
    "    # generation\n",
    "    #g = Graph.Tree(127, 2)\n",
    "    #g = Graph.GRG(100, 0.2)\n",
    "    #g = Graph.Erdos_Renyi(100, 0.2)\n",
    "    #g = Graph.Watts_Strogatz(1, 100, 4, 0.5 )\n",
    "    g = Graph.Barabasi(n_node)\n",
    "    summary(g)\n",
    "\n",
    "    # graph metrics\n",
    "    # pprint(g.degree([2,3,6,99]))\n",
    "    # pprint(g.edge_betweenness())\n",
    "    # pprint(g.pagerank())\n",
    "    #pprint(g.get_adjacency())\n",
    "    #pprint(dir(g))\n",
    "    \n",
    "\n",
    "\n",
    "    # test.index\n",
    "    testList = [] \n",
    "    with open(suffix+'.test.index','wb') as f:\n",
    "        while len(testList) < n_test:\n",
    "            j = randint(0, n_node-1)\n",
    "            #verify it is not alerady there\n",
    "            try:\n",
    "                pos = testList.index(j)\n",
    "            except:\n",
    "                testList.append(j)\n",
    "            f.write(str.encode(str(j)+\"\\n\"))\n",
    "\n",
    "\n",
    "    # adjacency dict (graph)\n",
    "    i = 0\n",
    "    A = g.get_adjlist()\n",
    "    graphAdj = {}\n",
    "    for node in A:\n",
    "        graphAdj[i] = node\n",
    "        i+=1\n",
    "    pickleToDisk(graphAdj, suffix+\".graph\")\n",
    "\n",
    "    \n",
    "    xFeatures = np.ones(n_node)\n",
    "    # allx is for labeled and unlabeled training samples\n",
    "    allx  = copy.deepcopy([xFeatures[i] for i in range(n_node) if i not in testList])\n",
    "    # x is for labeled training samples only\n",
    "    x = copy.deepcopy(allx[:-500])\n",
    "    #tx = copy.deepcopy(xFeatures[range(n_node-n_test, n_node)])\n",
    "    tx = [xFeatures[i] for i in testList]\n",
    "    \n",
    "\n",
    "    \n",
    "    allx = csr_matrix(allx)\n",
    "    allx = np.transpose(allx)\n",
    "    pickleToDisk(allx, suffix+\".allx\")\n",
    "    x = csr_matrix(x)\n",
    "    x = np.transpose(x)\n",
    "    pickleToDisk(x, suffix+\".x\")\n",
    "    tx = csr_matrix(tx)\n",
    "    tx = np.transpose(tx)\n",
    "    pickleToDisk(tx, suffix+\".tx\")\n",
    "\n",
    "    print(x.shape)\n",
    "    print(tx.shape)\n",
    "    print(allx.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    # labels (ally,y , ty)\n",
    "    prs = discretize(g.pagerank())\n",
    "\n",
    "    \n",
    "\n",
    "    # labels of x, and empty labels of unlabeled training samples\n",
    "    prs_all = copy.deepcopy([prs[i] for i in range(n_node) if i not in testList])\n",
    "    # labels of the labeled training samples\n",
    "    prs_train = copy.deepcopy(prs_all[:-500])\n",
    "    prs_test = copy.deepcopy([prs[i] for i in testList])\n",
    "    #testList.extend(validationList)\n",
    "    #finalList = testList\n",
    "    #prs_train = [ prs[i] for i in range(prs.shape[0]) if i not in finalList ]\n",
    "\n",
    "    ally = np.array(prs_all)\n",
    "    y = np.array(prs_train)  \n",
    "    ty = np.array(prs_test)\n",
    "\n",
    "\n",
    "    pickleToDisk(ally, suffix+\".ally\")\n",
    "    pickleToDisk(y, suffix+\".y\")\n",
    "    pickleToDisk(ty, suffix+\".ty\")\n",
    "    \n",
    "    print(y.shape)\n",
    "    print(ty.shape)\n",
    "    print(ally.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "    # run training on those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGRAPH U--- 4000 3999 -- \n",
      "(3200, 1)\n",
      "(300, 1)\n",
      "(3700, 1)\n",
      "0\n",
      "[ True  True  True ...  True  True  True]\n",
      "1\n",
      "[False False False ... False False False]\n",
      "2\n",
      "[False False False ... False False False]\n",
      "3\n",
      "[False False False ... False False False]\n",
      "4\n",
      "[False False False ... False False False]\n",
      "5\n",
      "[False False False ... False False False]\n",
      "6\n",
      "[False False False ... False False False]\n",
      "7\n",
      "[False False False ... False False False]\n",
      "8\n",
      "[False False False ... False False False]\n",
      "9\n",
      "[False False False ... False False False]\n",
      "(4000, 10)\n",
      "(3200, 10)\n",
      "(300, 10)\n",
      "(3700, 10)\n"
     ]
    }
   ],
   "source": [
    "generateData(suffix=\"ind.t3\",n_node=4000, n_train=700, n_test=300)\n",
    "!mv ind.* ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(y): 3200\n",
      "labels shape:\n",
      "(4000, 10)\n",
      "len(idx_train) 3200\n",
      "len(idx_val) 500\n",
      "len(idx_test) 310\n",
      "features after row normalize and tuple\n",
      "(array([[   0,    0],\n",
      "       [   1,    0],\n",
      "       [   2,    0],\n",
      "       ...,\n",
      "       [3997,    0],\n",
      "       [3998,    0],\n",
      "       [3999,    0]], dtype=int32),\n",
      " array([1., 1., 1., ..., 1., 1., 1.]),\n",
      " (4000, 1))\n",
      "WARNING:tensorflow:From /media/disk/home/pau/Projectes/CSN-proj/src/gcn/gcn_mod/gcn/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "2019-01-22 10:59:28.203330: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Epoch: 0001 train_loss= 2.40027 train_acc= 0.03344 val_loss= 2.33939 val_acc= 0.00000 time= 0.04083\n",
      "Epoch: 0002 train_loss= 2.36241 train_acc= 0.04688 val_loss= 2.31095 val_acc= 0.00000 time= 0.01175\n",
      "Epoch: 0003 train_loss= 2.32397 train_acc= 0.03469 val_loss= 2.28803 val_acc= 0.00000 time= 0.01226\n",
      "Epoch: 0004 train_loss= 2.29589 train_acc= 0.04312 val_loss= 2.26873 val_acc= 0.00000 time= 0.01080\n",
      "Epoch: 0005 train_loss= 2.27340 train_acc= 0.04469 val_loss= 2.25025 val_acc= 0.00000 time= 0.01167\n",
      "Epoch: 0006 train_loss= 2.25528 train_acc= 0.04219 val_loss= 2.23058 val_acc= 0.00000 time= 0.01085\n",
      "Epoch: 0007 train_loss= 2.23734 train_acc= 0.03906 val_loss= 2.20864 val_acc= 0.00000 time= 0.01146\n",
      "Epoch: 0008 train_loss= 2.21194 train_acc= 0.03750 val_loss= 2.18366 val_acc= 0.00000 time= 0.01093\n",
      "Epoch: 0009 train_loss= 2.18332 train_acc= 0.03969 val_loss= 2.15499 val_acc= 0.00000 time= 0.00980\n",
      "Epoch: 0010 train_loss= 2.16051 train_acc= 0.04062 val_loss= 2.12216 val_acc= 0.00000 time= 0.01064\n",
      "Epoch: 0011 train_loss= 2.13054 train_acc= 0.03719 val_loss= 2.08472 val_acc= 0.00000 time= 0.00965\n",
      "Epoch: 0012 train_loss= 2.09616 train_acc= 0.04969 val_loss= 2.04222 val_acc= 0.00000 time= 0.01117\n",
      "Epoch: 0013 train_loss= 2.05852 train_acc= 0.04344 val_loss= 1.99412 val_acc= 0.00000 time= 0.01076\n",
      "Epoch: 0014 train_loss= 2.02413 train_acc= 0.04375 val_loss= 1.93994 val_acc= 0.00000 time= 0.01011\n",
      "Epoch: 0015 train_loss= 1.95677 train_acc= 0.04094 val_loss= 1.87919 val_acc= 0.00000 time= 0.01185\n",
      "Epoch: 0016 train_loss= 1.91279 train_acc= 0.03562 val_loss= 1.81139 val_acc= 0.00000 time= 0.01006\n",
      "Epoch: 0017 train_loss= 1.84367 train_acc= 0.05125 val_loss= 1.73616 val_acc= 0.00000 time= 0.01244\n",
      "Epoch: 0018 train_loss= 1.79715 train_acc= 0.05344 val_loss= 1.65318 val_acc= 0.00000 time= 0.01207\n",
      "Epoch: 0019 train_loss= 1.70470 train_acc= 0.04688 val_loss= 1.56202 val_acc= 0.00000 time= 0.01108\n",
      "Epoch: 0020 train_loss= 1.60449 train_acc= 0.26937 val_loss= 1.46256 val_acc= 1.00000 time= 0.01137\n",
      "Epoch: 0021 train_loss= 1.52126 train_acc= 0.70438 val_loss= 1.35489 val_acc= 1.00000 time= 0.01142\n",
      "Epoch: 0022 train_loss= 1.41853 train_acc= 0.84625 val_loss= 1.23921 val_acc= 1.00000 time= 0.00975\n",
      "Epoch: 0023 train_loss= 1.27488 train_acc= 0.87063 val_loss= 1.11643 val_acc= 1.00000 time= 0.01001\n",
      "Epoch: 0024 train_loss= 1.25101 train_acc= 0.85812 val_loss= 0.98817 val_acc= 1.00000 time= 0.01019\n",
      "Epoch: 0025 train_loss= 1.14300 train_acc= 0.99875 val_loss= 0.85658 val_acc= 1.00000 time= 0.00993\n",
      "Epoch: 0026 train_loss= 0.94640 train_acc= 0.99844 val_loss= 0.72441 val_acc= 1.00000 time= 0.01216\n",
      "Epoch: 0027 train_loss= 0.90862 train_acc= 0.99937 val_loss= 0.59575 val_acc= 1.00000 time= 0.01004\n",
      "Epoch: 0028 train_loss= 0.81609 train_acc= 0.99969 val_loss= 0.47474 val_acc= 1.00000 time= 0.01146\n",
      "Epoch: 0029 train_loss= 0.70816 train_acc= 0.99969 val_loss= 0.36577 val_acc= 1.00000 time= 0.01004\n",
      "Epoch: 0030 train_loss= 0.64357 train_acc= 1.00000 val_loss= 0.27197 val_acc= 1.00000 time= 0.01144\n",
      "Epoch: 0031 train_loss= 0.54460 train_acc= 1.00000 val_loss= 0.19528 val_acc= 1.00000 time= 0.01179\n",
      "Epoch: 0032 train_loss= 0.48620 train_acc= 1.00000 val_loss= 0.13555 val_acc= 1.00000 time= 0.01020\n",
      "Epoch: 0033 train_loss= 0.43862 train_acc= 1.00000 val_loss= 0.09119 val_acc= 1.00000 time= 0.01038\n",
      "Epoch: 0034 train_loss= 0.36280 train_acc= 0.99969 val_loss= 0.05967 val_acc= 1.00000 time= 0.01014\n",
      "Epoch: 0035 train_loss= 0.34114 train_acc= 1.00000 val_loss= 0.03816 val_acc= 1.00000 time= 0.01072\n",
      "Epoch: 0036 train_loss= 0.33689 train_acc= 1.00000 val_loss= 0.02398 val_acc= 1.00000 time= 0.01312\n",
      "Epoch: 0037 train_loss= 0.29197 train_acc= 1.00000 val_loss= 0.01492 val_acc= 1.00000 time= 0.01145\n",
      "Epoch: 0038 train_loss= 0.24728 train_acc= 1.00000 val_loss= 0.00929 val_acc= 1.00000 time= 0.01011\n",
      "Epoch: 0039 train_loss= 0.22650 train_acc= 1.00000 val_loss= 0.00585 val_acc= 1.00000 time= 0.01169\n",
      "Epoch: 0040 train_loss= 0.23681 train_acc= 0.99969 val_loss= 0.00380 val_acc= 1.00000 time= 0.01146\n",
      "Epoch: 0041 train_loss= 0.19627 train_acc= 1.00000 val_loss= 0.00258 val_acc= 1.00000 time= 0.01142\n",
      "Epoch: 0042 train_loss= 0.17347 train_acc= 1.00000 val_loss= 0.00186 val_acc= 1.00000 time= 0.01077\n",
      "Epoch: 0043 train_loss= 0.19704 train_acc= 1.00000 val_loss= 0.00145 val_acc= 1.00000 time= 0.01113\n",
      "Epoch: 0044 train_loss= 0.19451 train_acc= 1.00000 val_loss= 0.00121 val_acc= 1.00000 time= 0.01148\n",
      "Epoch: 0045 train_loss= 0.17599 train_acc= 1.00000 val_loss= 0.00107 val_acc= 1.00000 time= 0.00992\n",
      "Epoch: 0046 train_loss= 0.14196 train_acc= 1.00000 val_loss= 0.00100 val_acc= 1.00000 time= 0.01004\n",
      "Epoch: 0047 train_loss= 0.16836 train_acc= 1.00000 val_loss= 0.00096 val_acc= 1.00000 time= 0.01012\n",
      "Epoch: 0048 train_loss= 0.13487 train_acc= 1.00000 val_loss= 0.00095 val_acc= 1.00000 time= 0.01049\n",
      "Epoch: 0049 train_loss= 0.17982 train_acc= 1.00000 val_loss= 0.00094 val_acc= 1.00000 time= 0.01172\n",
      "Epoch: 0050 train_loss= 0.15030 train_acc= 1.00000 val_loss= 0.00094 val_acc= 1.00000 time= 0.01020\n",
      "Epoch: 0051 train_loss= 0.10761 train_acc= 1.00000 val_loss= 0.00095 val_acc= 1.00000 time= 0.00978\n",
      "Epoch: 0052 train_loss= 0.14131 train_acc= 1.00000 val_loss= 0.00096 val_acc= 1.00000 time= 0.01016\n",
      "Epoch: 0053 train_loss= 0.14855 train_acc= 1.00000 val_loss= 0.00097 val_acc= 1.00000 time= 0.01089\n",
      "Epoch: 0054 train_loss= 0.12761 train_acc= 1.00000 val_loss= 0.00097 val_acc= 1.00000 time= 0.01011\n",
      "Epoch: 0055 train_loss= 0.13359 train_acc= 1.00000 val_loss= 0.00098 val_acc= 1.00000 time= 0.00976\n",
      "Early stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.00098 accuracy= 1.00000 time= 0.00373\n",
      "array([[-69.26271  , -88.4687   ],\n",
      "       [-74.15443  , -99.7186   ],\n",
      "       [-22.539757 , -31.586008 ],\n",
      "       ...,\n",
      "       [ -3.83033  ,  -5.2300115],\n",
      "       [-13.480366 , -15.678567 ],\n",
      "       [-12.42572  , -18.634624 ]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "!~/.pyenv/shims/python  train.py --dataset t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(y): 140\n",
      "labels shape:\n",
      "(2708, 7)\n",
      "len(idx_train) 140\n",
      "len(idx_val) 500\n",
      "len(idx_test) 1000\n",
      "features after row normalize and tuple\n",
      "(array([[   0, 1274],\n",
      "       [   0, 1247],\n",
      "       [   0, 1194],\n",
      "       ...,\n",
      "       [2707,  329],\n",
      "       [2707,  186],\n",
      "       [2707,   19]], dtype=int32),\n",
      " array([0.11111111, 0.11111111, 0.11111111, ..., 0.07692308, 0.07692308,\n",
      "       0.07692308], dtype=float32),\n",
      " (2708, 1433))\n",
      "WARNING:tensorflow:From /media/disk/home/pau/Projectes/CSN-proj/src/gcn/gcn_mod/gcn/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "2019-01-22 11:04:21.995226: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Epoch: 0001 train_loss= 1.95369 train_acc= 0.17857 val_loss= 1.94959 val_acc= 0.19600 time= 0.05467\n",
      "Epoch: 0002 train_loss= 1.94921 train_acc= 0.22857 val_loss= 1.94556 val_acc= 0.25000 time= 0.01941\n",
      "Epoch: 0003 train_loss= 1.94375 train_acc= 0.25714 val_loss= 1.94130 val_acc= 0.26600 time= 0.02109\n",
      "Epoch: 0004 train_loss= 1.93951 train_acc= 0.22143 val_loss= 1.93656 val_acc= 0.27000 time= 0.01901\n",
      "Epoch: 0005 train_loss= 1.93176 train_acc= 0.26429 val_loss= 1.93142 val_acc= 0.27600 time= 0.01964\n",
      "Epoch: 0006 train_loss= 1.92502 train_acc= 0.26429 val_loss= 1.92582 val_acc= 0.28800 time= 0.01848\n",
      "Epoch: 0007 train_loss= 1.91947 train_acc= 0.26429 val_loss= 1.91962 val_acc= 0.29400 time= 0.01831\n",
      "Epoch: 0008 train_loss= 1.90527 train_acc= 0.28571 val_loss= 1.91259 val_acc= 0.30000 time= 0.01915\n",
      "Epoch: 0009 train_loss= 1.90319 train_acc= 0.26429 val_loss= 1.90475 val_acc= 0.30000 time= 0.01854\n",
      "Epoch: 0010 train_loss= 1.88612 train_acc= 0.27143 val_loss= 1.89592 val_acc= 0.30400 time= 0.02158\n",
      "Epoch: 0011 train_loss= 1.87968 train_acc= 0.30714 val_loss= 1.88595 val_acc= 0.30800 time= 0.01925\n",
      "Epoch: 0012 train_loss= 1.87549 train_acc= 0.25714 val_loss= 1.87492 val_acc= 0.31200 time= 0.01844\n",
      "Epoch: 0013 train_loss= 1.85802 train_acc= 0.27143 val_loss= 1.86275 val_acc= 0.31600 time= 0.01856\n",
      "Epoch: 0014 train_loss= 1.83655 train_acc= 0.27857 val_loss= 1.84932 val_acc= 0.31400 time= 0.01905\n",
      "Epoch: 0015 train_loss= 1.82313 train_acc= 0.26429 val_loss= 1.83487 val_acc= 0.31600 time= 0.01905\n",
      "Epoch: 0016 train_loss= 1.81615 train_acc= 0.30000 val_loss= 1.81921 val_acc= 0.32800 time= 0.02043\n",
      "Epoch: 0017 train_loss= 1.76850 train_acc= 0.28571 val_loss= 1.80243 val_acc= 0.32400 time= 0.01930\n",
      "Epoch: 0018 train_loss= 1.76992 train_acc= 0.29286 val_loss= 1.78469 val_acc= 0.32200 time= 0.01951\n",
      "Epoch: 0019 train_loss= 1.76787 train_acc= 0.29286 val_loss= 1.76595 val_acc= 0.32400 time= 0.01987\n",
      "Epoch: 0020 train_loss= 1.74701 train_acc= 0.30000 val_loss= 1.74612 val_acc= 0.33000 time= 0.01929\n",
      "Epoch: 0021 train_loss= 1.70119 train_acc= 0.29286 val_loss= 1.72515 val_acc= 0.32600 time= 0.02005\n",
      "Epoch: 0022 train_loss= 1.67643 train_acc= 0.32143 val_loss= 1.70351 val_acc= 0.32200 time= 0.02185\n",
      "Epoch: 0023 train_loss= 1.64302 train_acc= 0.33571 val_loss= 1.68135 val_acc= 0.32600 time= 0.02163\n",
      "Epoch: 0024 train_loss= 1.68050 train_acc= 0.29286 val_loss= 1.65940 val_acc= 0.32400 time= 0.02337\n",
      "Epoch: 0025 train_loss= 1.62081 train_acc= 0.30714 val_loss= 1.63768 val_acc= 0.32600 time= 0.02131\n",
      "Epoch: 0026 train_loss= 1.58796 train_acc= 0.31429 val_loss= 1.61674 val_acc= 0.32800 time= 0.02013\n",
      "Epoch: 0027 train_loss= 1.60783 train_acc= 0.28571 val_loss= 1.59668 val_acc= 0.33200 time= 0.02062\n",
      "Epoch: 0028 train_loss= 1.55734 train_acc= 0.35714 val_loss= 1.57728 val_acc= 0.33200 time= 0.01865\n",
      "Epoch: 0029 train_loss= 1.53634 train_acc= 0.31429 val_loss= 1.55883 val_acc= 0.33200 time= 0.01880\n",
      "Epoch: 0030 train_loss= 1.53152 train_acc= 0.33571 val_loss= 1.54148 val_acc= 0.33600 time= 0.02045\n",
      "Epoch: 0031 train_loss= 1.48951 train_acc= 0.30714 val_loss= 1.52507 val_acc= 0.33600 time= 0.01858\n",
      "Epoch: 0032 train_loss= 1.51346 train_acc= 0.30714 val_loss= 1.50947 val_acc= 0.34400 time= 0.01838\n",
      "Epoch: 0033 train_loss= 1.47774 train_acc= 0.34286 val_loss= 1.49471 val_acc= 0.34800 time= 0.02062\n",
      "Epoch: 0034 train_loss= 1.49114 train_acc= 0.34286 val_loss= 1.48132 val_acc= 0.35000 time= 0.02139\n",
      "Epoch: 0035 train_loss= 1.46085 train_acc= 0.31429 val_loss= 1.46899 val_acc= 0.35400 time= 0.01891\n",
      "Epoch: 0036 train_loss= 1.45599 train_acc= 0.36429 val_loss= 1.45690 val_acc= 0.36000 time= 0.01972\n",
      "Epoch: 0037 train_loss= 1.40535 train_acc= 0.35714 val_loss= 1.44363 val_acc= 0.36000 time= 0.01885\n",
      "Epoch: 0038 train_loss= 1.47875 train_acc= 0.35000 val_loss= 1.43247 val_acc= 0.36600 time= 0.02083\n",
      "Epoch: 0039 train_loss= 1.38573 train_acc= 0.39286 val_loss= 1.42188 val_acc= 0.36800 time= 0.02009\n",
      "Epoch: 0040 train_loss= 1.37480 train_acc= 0.38571 val_loss= 1.41103 val_acc= 0.36800 time= 0.01936\n",
      "Epoch: 0041 train_loss= 1.37847 train_acc= 0.36429 val_loss= 1.40098 val_acc= 0.37000 time= 0.02091\n",
      "Epoch: 0042 train_loss= 1.37917 train_acc= 0.42857 val_loss= 1.39279 val_acc= 0.39200 time= 0.01901\n",
      "Epoch: 0043 train_loss= 1.35797 train_acc= 0.38571 val_loss= 1.38634 val_acc= 0.41800 time= 0.01948\n",
      "Epoch: 0044 train_loss= 1.37528 train_acc= 0.40000 val_loss= 1.38038 val_acc= 0.41400 time= 0.01915\n",
      "Epoch: 0045 train_loss= 1.32314 train_acc= 0.40000 val_loss= 1.37092 val_acc= 0.42200 time= 0.01966\n",
      "Epoch: 0046 train_loss= 1.23770 train_acc= 0.43571 val_loss= 1.36257 val_acc= 0.43200 time= 0.01963\n",
      "Epoch: 0047 train_loss= 1.27809 train_acc= 0.40000 val_loss= 1.35307 val_acc= 0.44000 time= 0.02209\n",
      "Epoch: 0048 train_loss= 1.31789 train_acc= 0.42857 val_loss= 1.34353 val_acc= 0.44400 time= 0.01951\n",
      "Epoch: 0049 train_loss= 1.24131 train_acc= 0.47857 val_loss= 1.33493 val_acc= 0.46000 time= 0.01889\n",
      "Epoch: 0050 train_loss= 1.25496 train_acc= 0.42143 val_loss= 1.32762 val_acc= 0.46400 time= 0.01848\n",
      "Epoch: 0051 train_loss= 1.21900 train_acc= 0.50000 val_loss= 1.31945 val_acc= 0.47600 time= 0.01889\n",
      "Epoch: 0052 train_loss= 1.25635 train_acc= 0.44286 val_loss= 1.31175 val_acc= 0.49000 time= 0.01901\n",
      "Epoch: 0053 train_loss= 1.20935 train_acc= 0.42857 val_loss= 1.30395 val_acc= 0.50200 time= 0.01801\n",
      "Epoch: 0054 train_loss= 1.19416 train_acc= 0.54286 val_loss= 1.29537 val_acc= 0.51600 time= 0.02017\n",
      "Epoch: 0055 train_loss= 1.23380 train_acc= 0.45714 val_loss= 1.28695 val_acc= 0.53000 time= 0.02107\n",
      "Epoch: 0056 train_loss= 1.12606 train_acc= 0.60000 val_loss= 1.27787 val_acc= 0.53600 time= 0.02006\n",
      "Epoch: 0057 train_loss= 1.17065 train_acc= 0.53571 val_loss= 1.27092 val_acc= 0.55200 time= 0.02232\n",
      "Epoch: 0058 train_loss= 1.10761 train_acc= 0.58571 val_loss= 1.26259 val_acc= 0.56400 time= 0.01974\n",
      "Epoch: 0059 train_loss= 1.16406 train_acc= 0.57857 val_loss= 1.25128 val_acc= 0.57000 time= 0.01912\n",
      "Epoch: 0060 train_loss= 1.14564 train_acc= 0.57143 val_loss= 1.24129 val_acc= 0.57800 time= 0.02126\n",
      "Epoch: 0061 train_loss= 1.11760 train_acc= 0.61429 val_loss= 1.23227 val_acc= 0.59400 time= 0.01986\n",
      "Epoch: 0062 train_loss= 1.10711 train_acc= 0.54286 val_loss= 1.22476 val_acc= 0.60800 time= 0.01975\n",
      "Epoch: 0063 train_loss= 1.14884 train_acc= 0.58571 val_loss= 1.21843 val_acc= 0.61600 time= 0.01919\n",
      "Epoch: 0064 train_loss= 1.12463 train_acc= 0.57857 val_loss= 1.21317 val_acc= 0.64400 time= 0.02083\n",
      "Epoch: 0065 train_loss= 1.05123 train_acc= 0.65714 val_loss= 1.20802 val_acc= 0.64400 time= 0.02168\n",
      "Epoch: 0066 train_loss= 1.09774 train_acc= 0.62857 val_loss= 1.20044 val_acc= 0.65000 time= 0.02024\n",
      "Epoch: 0067 train_loss= 1.12759 train_acc= 0.60000 val_loss= 1.19363 val_acc= 0.65400 time= 0.01991\n",
      "Epoch: 0068 train_loss= 1.02748 train_acc= 0.71429 val_loss= 1.18427 val_acc= 0.65200 time= 0.02133\n",
      "Epoch: 0069 train_loss= 1.13527 train_acc= 0.58571 val_loss= 1.17472 val_acc= 0.65600 time= 0.02169\n",
      "Epoch: 0070 train_loss= 1.09586 train_acc= 0.65714 val_loss= 1.16718 val_acc= 0.64400 time= 0.02103\n",
      "Epoch: 0071 train_loss= 1.00209 train_acc= 0.63571 val_loss= 1.16038 val_acc= 0.64600 time= 0.01984\n",
      "Epoch: 0072 train_loss= 1.05120 train_acc= 0.64286 val_loss= 1.15422 val_acc= 0.66200 time= 0.02010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0073 train_loss= 1.13352 train_acc= 0.66429 val_loss= 1.14714 val_acc= 0.66600 time= 0.01889\n",
      "Epoch: 0074 train_loss= 1.01581 train_acc= 0.62857 val_loss= 1.14444 val_acc= 0.65800 time= 0.02054\n",
      "Epoch: 0075 train_loss= 0.99083 train_acc= 0.62143 val_loss= 1.14228 val_acc= 0.66200 time= 0.02001\n",
      "Epoch: 0076 train_loss= 1.05695 train_acc= 0.60000 val_loss= 1.14428 val_acc= 0.66000 time= 0.01973\n",
      "Epoch: 0077 train_loss= 1.06611 train_acc= 0.62857 val_loss= 1.14551 val_acc= 0.65200 time= 0.02008\n",
      "Epoch: 0078 train_loss= 1.04987 train_acc= 0.62143 val_loss= 1.14293 val_acc= 0.65200 time= 0.02045\n",
      "Epoch: 0079 train_loss= 0.93315 train_acc= 0.70000 val_loss= 1.13907 val_acc= 0.65200 time= 0.01960\n",
      "Epoch: 0080 train_loss= 0.92226 train_acc= 0.75000 val_loss= 1.13974 val_acc= 0.64400 time= 0.01865\n",
      "Epoch: 0081 train_loss= 1.02945 train_acc= 0.61429 val_loss= 1.14003 val_acc= 0.64400 time= 0.01806\n",
      "Epoch: 0082 train_loss= 0.93219 train_acc= 0.70000 val_loss= 1.14038 val_acc= 0.64400 time= 0.01873\n",
      "Epoch: 0083 train_loss= 0.89969 train_acc= 0.72143 val_loss= 1.13858 val_acc= 0.63800 time= 0.01799\n",
      "Epoch: 0084 train_loss= 0.92550 train_acc= 0.71429 val_loss= 1.13948 val_acc= 0.64200 time= 0.01826\n",
      "Epoch: 0085 train_loss= 0.96616 train_acc= 0.70714 val_loss= 1.13364 val_acc= 0.64000 time= 0.01844\n",
      "Epoch: 0086 train_loss= 0.96750 train_acc= 0.64286 val_loss= 1.12843 val_acc= 0.63800 time= 0.01863\n",
      "Epoch: 0087 train_loss= 0.92896 train_acc= 0.65714 val_loss= 1.12707 val_acc= 0.63800 time= 0.01882\n",
      "Epoch: 0088 train_loss= 0.94086 train_acc= 0.69286 val_loss= 1.12712 val_acc= 0.63800 time= 0.01977\n",
      "Epoch: 0089 train_loss= 0.91465 train_acc= 0.68571 val_loss= 1.13078 val_acc= 0.63400 time= 0.02053\n",
      "Epoch: 0090 train_loss= 0.91973 train_acc= 0.64286 val_loss= 1.13394 val_acc= 0.63000 time= 0.01961\n",
      "Epoch: 0091 train_loss= 0.90283 train_acc= 0.67857 val_loss= 1.13664 val_acc= 0.62400 time= 0.02026\n",
      "Early stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 1.17974 accuracy= 0.61000 time= 0.00933\n",
      "array([[-5.018782 , -1.0852845],\n",
      "       [-1.1500423,  1.9082782],\n",
      "       [-3.1022313,  1.3378185],\n",
      "       ...,\n",
      "       [-3.214873 , -2.1315284],\n",
      "       [-7.0004706, -2.673078 ],\n",
      "       [-6.470511 , -2.3801403]], dtype=float32)\n",
      "(2708, 7)\n",
      "(2708, 7)\n",
      "(2708, 7)\n"
     ]
    }
   ],
   "source": [
    "!~/.pyenv/shims/python  train.py --dataset cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-df19e22d8529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mplot2DEmbedding_oldd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"embedding_labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-186-df19e22d8529>\u001b[0m in \u001b[0;36mplot2DEmbedding_oldd\u001b[0;34m(filepath, labelspath)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def plot2DEmbedding_oldd(filepath, labelspath):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    import matplotlib.lines as mlines\n",
    "    %matplotlib inline\n",
    "\n",
    "    # read embedding labels file\n",
    "    with open(labelspath,\"r\") as f:\n",
    "        size = len(f.readlines())\n",
    "        labels = np.zeros((size,1))\n",
    "\n",
    "    # read embedding file\n",
    "    with open(filepath,\"r\") as f:\n",
    "        size = len(f.readlines())\n",
    "\n",
    "        \n",
    "    # read embedding labels file\n",
    "    embeddings = {}\n",
    "    with open(labelspath,\"r\") as f:\n",
    "        i=0\n",
    "        for label in f.readlines():\n",
    "            #print(line)\n",
    "            vals = label.split()\n",
    "            label_int = 0\n",
    "            for j in range(len(vals)):\n",
    "                label_int += float(vals[j])*j\n",
    "            labels[i]=int(label_int)\n",
    "            i=i+1\n",
    "            \n",
    "        \n",
    "        for j in set(labels.tolist()):\n",
    "            embeddings[j]=np.zeros((size,2))\n",
    "            \n",
    "        with open(filepath,\"r\") as f2:\n",
    "            i = 0\n",
    "            for line in f2.readlines():\n",
    "                #print(line)\n",
    "                d1,d2 = line.split()\n",
    "                embedding = embeddings[labels[j]]\n",
    "                np.append(embedding, [floag(d1), float(d2)], axis=0)\n",
    "                i=i+1\n",
    "        \n",
    "        \n",
    "        print(labels)\n",
    "        pprint(embeddings)\n",
    "\n",
    "        # separate embedding by label\n",
    "        \n",
    "        \n",
    "        color_labels = [int(k % 23) for k in labels]\n",
    "        for col in color_labels:\n",
    "            plt.scatter(embeddings[col][:,0], embedding[col][:,1], c=col)\n",
    "        plt.suptitle('Embedding')\n",
    "        plt.xlabel('dim 1')\n",
    "        plt.ylabel('dim 2')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "def plot2DEmbedding(filepath, labelspath):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    import matplotlib.lines as mlines\n",
    "    %matplotlib inline\n",
    "\n",
    "    # read embedding labels file\n",
    "    with open(labelspath,\"r\") as f:\n",
    "        size = len(f.readlines())\n",
    "        labels = np.zeros((size,1))\n",
    "    # read embedding labels file\n",
    "    with open(labelspath,\"r\") as f:\n",
    "        i = 0\n",
    "        for line in f.readlines():\n",
    "            #print(line)\n",
    "            vals = line.split()\n",
    "            label_int = 0\n",
    "            for j in range(len(vals)):\n",
    "                label_int += float(vals[j])*j\n",
    "            labels[i]=int(label_int)\n",
    "            i=i+1\n",
    "        \n",
    "    print(labels)\n",
    "    \n",
    "    # read embedding file\n",
    "    embedding = np.zeros((1,1))\n",
    "    with open(filepath,\"r\") as f:\n",
    "        size = len(f.readlines())\n",
    "        embedding = np.zeros((size,2))\n",
    "\n",
    "\n",
    "    with open(filepath,\"r\") as f:\n",
    "        i = 0\n",
    "        for line in f.readlines():\n",
    "            #print(line)\n",
    "            d1,d2 = line.split()\n",
    "            embedding[i,0] = float(d1)\n",
    "            embedding[i,1] = float(d2)\n",
    "            i=i+1\n",
    "\n",
    "        #print(embedding.shape)\n",
    "        #print(embedding[:,0])\n",
    "        #print(embedding[:,1]) \n",
    "        \n",
    "        # separate embedding by label\n",
    "        \n",
    "        \n",
    "        color_labels = [int(k % 23) for k in labels]\n",
    "        plt.scatter(embedding[:,0], embedding[:,1], c=color_labels)\n",
    "        plt.suptitle('Embedding')\n",
    "        plt.xlabel('dim 1')\n",
    "        plt.ylabel('dim 2')\n",
    "        \n",
    "        myhandles = []\n",
    "        for thecolor in set(color_labels):   \n",
    "            print(thecolor)\n",
    "            myhandles.append( mpatches.Patch( color=str(thecolor), label=thecolor))\n",
    "\n",
    "        plt.legend(handles=myhandles)\n",
    "        plt.show()\n",
    "        \n",
    "plot2DEmbedding_oldd(\"embedding\",\"embedding_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.  1.  1. ]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-ff65d704cdd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mplot2DEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"embedding_labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-250-ff65d704cdd4>\u001b[0m in \u001b[0;36mplot2DEmbedding\u001b[0;34m(filepath, labelspath)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m#print([[col]*embeddings[col].shape[0]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m#col=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot2DEmbedding(filepath, labelspath):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    import matplotlib.lines as mlines\n",
    "    import matplotlib.cm as cm\n",
    "    %matplotlib inline\n",
    "\n",
    "    # read embedding labels file\n",
    "    with open(labelspath,\"r\") as f:\n",
    "        size = len(f.readlines())\n",
    "        labels = np.zeros((size,1))\n",
    "\n",
    "    # read embedding file\n",
    "    with open(filepath,\"r\") as f:\n",
    "        size = len(f.readlines())\n",
    "\n",
    "        \n",
    "    # read embedding labels file\n",
    "    embeddings = {}\n",
    "    with open(labelspath,\"r\") as f:\n",
    "        i=0\n",
    "        for label in f.readlines():\n",
    "            #print(line)\n",
    "            vals = label.split()\n",
    "            label_int = 0\n",
    "            for j in range(len(vals)):\n",
    "                label_int += float(vals[j])*j\n",
    "            labels[i]=int(label_int)\n",
    "            i=i+1\n",
    "            \n",
    "        labels_list = [ lab[0] for lab in labels.tolist()]\n",
    "        for j in set(labels_list):\n",
    "            embeddings[j]=np.empty((0,2))\n",
    "            \n",
    "        with open(filepath,\"r\") as f2:\n",
    "            i = 0\n",
    "            for line in f2.readlines():\n",
    "                #print(line)\n",
    "                d1,d2 = line.split()\n",
    "                key = int(labels_list[i])\n",
    "                embedding = embeddings[key]\n",
    "                \n",
    "                embeddings[key] = np.append(embedding, [[float(d1), float(d2)]], axis=0)\n",
    "                #embedding[i,0]=float(d1)\n",
    "                #embedding[i,1]=float(d2)\n",
    "                i=i+1\n",
    "        \n",
    "        # separate embedding by label\n",
    "        \n",
    "        color_labels = [int(k % 23) for k in set(labels_list) ]\n",
    "        colors = cm.rainbow(np.linspace(0, 1, len(color_labels)))\n",
    "        for colref,col in zip(colors, color_labels) :\n",
    "            print(col)\n",
    "            #print([[col]*embeddings[col].shape[0]])\n",
    "            print(embeddings[col].shape[0])\n",
    "            plt.scatter(embeddings[col][:,0], embeddings[col][:,1], c=[col]*embeddings[col].shape[0], label=str(colabel))\n",
    "        #col=0\n",
    "        #plt.scatter(embeddings[col][:,0], embeddings[col][:,1], c=str(col), label=str(col))\n",
    "        plt.suptitle('Embedding')\n",
    "        plt.xlabel('dim 1')\n",
    "        plt.ylabel('dim 2')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "   \n",
    "        \n",
    "plot2DEmbedding(\"embedding\",\"embedding_labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csnProjPy3",
   "language": "python",
   "name": "csnprojpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
